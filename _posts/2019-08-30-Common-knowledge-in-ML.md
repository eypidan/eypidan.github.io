---
layout: post
title:  "Common knowledge in ML"
author: "eypidan"
use_math: true

---

## Loss Function
- Cross-Entropy

### Cross-Entropy

- Cross-entropy loss : log loss
- 

$$
\begin{aligned} \dot{x} &= \sigma(y-x) \\ 
\dot{y} &= \rho x - y - xz \\ 
\dot{z} &= -\beta z + xy \end{aligned}
$$

